{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Project: Pet Classifier using CNN\n",
    "\n",
    "Prepration\n",
    "- Extract the ipynb file and the data in the same folder\n",
    "\n",
    "Data Set\n",
    "- A production grade program as 10,000 training images\n",
    "- This is a small program with 20 images of cats and 20 images of dogs. \n",
    "- The evaluation set has 10 images of cats and 10 images of dogs\n",
    "\n",
    "Runs\n",
    "- The student is expected to run the 100-300 training step\n",
    "- A production grade code would have about 20k-50k training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyper parameters\n",
    "- Run the program with three num_steps : 100,200,300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "img_size = 32\n",
    "num_channels = 3\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "img_shape = (img_size, img_size)\n",
    "trainpath='C:/tensorflow/train'\n",
    "testpath='C:/tensorflow/test'\n",
    "labels = {'cats': 0, 'dogs': 1}\n",
    "fc_size=32 #size of the output of final FC layer\n",
    "num_steps=300 #Try 100, 200, 300. number of steps that training data should be looped. Usually 20K\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train image set 40\n",
      "X_data shape: (40, 32, 32, 3)\n",
      "y_data shape: (40,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_images_classes(basepath,imgSize=img_size):\n",
    "    image_stack = []\n",
    "    label_stack = []\n",
    "\n",
    "    for counter, l in enumerate(labels):\n",
    "        path = os.path.join(basepath, l,'*g')\n",
    "        for img in glob.glob(path):\n",
    "            one_hot_vector =np.zeros(len(labels),dtype=np.int16)\n",
    "            one_hot_vector[counter]=1\n",
    "            image = cv2.imread(img)\n",
    "            im_resize = cv2.resize(image,img_shape, interpolation=cv2.INTER_CUBIC)\n",
    "            image_stack.append(im_resize)\n",
    "            label_stack.append(labels[l])            \n",
    "    return np.array(image_stack), np.array(label_stack)\n",
    "\n",
    "X_train, y_train=read_images_classes(trainpath)\n",
    "X_test, y_test=read_images_classes(testpath)\n",
    "\n",
    "#test a sample image\n",
    "print('length of train image set',len(X_train))\n",
    "print('X_data shape:', X_train.shape)\n",
    "print('y_data shape:', y_train.shape)\n",
    "\n",
    "fig1 = plt.figure() \n",
    "ax1 = fig1.add_subplot(2,2,1) \n",
    "img = cv2.resize(X_train[0],(64,64), interpolation=cv2.INTER_CUBIC)\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(y_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Define the tensorflow model\n",
    "\n",
    "The model should have the following layers\n",
    "- input later\n",
    "- conv layer 1 with 32 filters of kernel  size[5,5],\n",
    "- pooling layer 1 with pool size[2,2] and stride 2\n",
    "- conv layer 2 with 64 filters of kernel  size[5,5],\n",
    "- pooling layer 2 with pool size[2,2] and stride 2\n",
    "- dense layer whose output size is fixed in the hyper parameter: fc_size=32\n",
    "- drop out layer with droput probability 0.4\n",
    "- predict the class by doing a softmax on the output of the dropout layers\n",
    "\n",
    "Training\n",
    "- For training fefine the loss function and minimize it\n",
    "- For evaluation calculate the accuracy\n",
    "\n",
    "Reading Material\n",
    "- For ideas look at tensorflow layers tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cnn_model_fn has to be defined here by the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "\n",
    "# Input Layer\n",
    "    input_layer = tf.reshape(features[\"image\"], [-1, _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, 3])\n",
    " \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    " \n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    " \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    " \n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 126 * 126 * 32])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    " \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tensorflow model\n",
    "\n",
    "This section will use the model defined by the student and run the training and evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # softmax for prediction \n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    " \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def one_hot_encode(x):\n",
    "    \n",
    "    lb.fit(x)\n",
    "    lb.classes_ = list(range(0, 1))\n",
    "    return lb.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-7-c0461f5330f9>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-c0461f5330f9>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    loss = tf.losses.softmax_cross_entropy(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# defining the loss function. \n",
    "tf.reset_default_graph()\n",
    "   \n",
    " loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=onehot_labels,logits=logits)\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.001)\n",
    "                          \n",
    "  train_op = optimizer.minimize(loss=cost)\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-70aff7fdde26>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-70aff7fdde26>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "   optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "   train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-36-4b72483da3a2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-4b72483da3a2>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    return tf.estimator.EstimatorSpec(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/pets_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000040055AD4A8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-488dc1877004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": X_train}, y=y_train, batch_size=10,\n\u001b[0;32m     11\u001b[0m                                                       num_epochs=None, shuffle=True)\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpets_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogging_hook\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0meval_input_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpets_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1152\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[1;32m-> 1154\u001b[1;33m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[0;32m   1155\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-19013982718a>\u001b[0m in \u001b[0;36mcnn_model_fn\u001b[1;34m(features, labels, mode)\u001b[0m\n\u001b[0;32m      3\u001b[0m     predictions = {\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Generate predictions (for PREDICT and EVAL mode)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;34m\"classes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Add `softmax_tensor` to the graph. It is used for PREDICT and by the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# `logging_hook`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "#X_train = np.array((X_train/255.0),dtype=np.float16)\n",
    "#X_test = np.array((X_test/255.0), dtype=np.float16)\n",
    "X_train = np.array((X_train/255.0),dtype=np.float32)\n",
    "X_test = np.array((X_test/255.0), dtype=np.float32)\n",
    "\n",
    "pets_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/pets_convnet_model\")\n",
    "#pets_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": X_train}, y=y_train, batch_size=10,\n",
    "                                                      num_epochs=None, shuffle=True)\n",
    "pets_classifier.train(input_fn=train_input_fn, steps=num_steps, hooks=[logging_hook])\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": X_test}, y=y_test, num_epochs=1,shuffle=False)\n",
    "eval_results = pets_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-97-c6086bd0bd4f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-97-c6086bd0bd4f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Calculate Loss (for both TRAIN and EVAL modes)\n",
    "onehot_labels = tf.one_hot(indices=tf.cast(labels,tf.type=int32, depth=2)\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lists(data_dir):\n",
    "    import glob\n",
    " \n",
    "    train_list = glob.glob(data_dir + '/' + 'train-*')\n",
    "    valid_list = glob.glob(data_dir + '/' + 'validation-*')\n",
    "    if len(train_list) == 0 and \\\n",
    "                    len(valid_list) == 0:\n",
    "        raise IOError('No files found at specified path!')\n",
    "    return train_list, valid_list\n",
    " \n",
    "def parse_record(raw_record, is_training):\n",
    "    \"\"\"Parse an ImageNet record from `value`.\"\"\"\n",
    "    keys_to_features = {\n",
    "        'image/encoded':\n",
    "            tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format':\n",
    "            tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n",
    "        'image/class/label':\n",
    "            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n",
    "        'image/class/text':\n",
    "            tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "    }\n",
    " \n",
    "    parsed = tf.parse_single_example(raw_record, keys_to_features)\n",
    " \n",
    "    image = tf.image.decode_image(\n",
    "        tf.reshape(parsed['image/encoded'], shape=[]),\n",
    "        _NUM_CHANNELS)\n",
    " \n",
    "    # Note that tf.image.convert_image_dtype scales the image data to [0, 1).\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    " \n",
    "    image = vgg_preprocessing.preprocess_image(\n",
    "        image=image,\n",
    "        output_height=_DEFAULT_IMAGE_SIZE,\n",
    "        output_width=_DEFAULT_IMAGE_SIZE,\n",
    "        is_training=is_training)\n",
    " \n",
    "    label = tf.cast(\n",
    "        tf.reshape(parsed['image/class/label'], shape=[]),\n",
    "        dtype=tf.int32)\n",
    " \n",
    "    return {\"image\": image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(is_training, filenames, batch_size, num_epochs=1, num_parallel_calls=1):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    " \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1500)\n",
    " \n",
    "    dataset = dataset.map(lambda value: parse_record(value, is_training),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    " \n",
    "    features, labels = iterator.get_next()\n",
    " \n",
    "    return features, labels\n",
    " \n",
    " \n",
    "def train_input_fn(file_path):\n",
    "    return input_fn(True, file_path, 100, None, 10)\n",
    " \n",
    " \n",
    "def validation_input_fn(file_path):\n",
    "    return input_fn(False, file_path, 50, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/convnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "  logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=lambda: train_input_fn(train_list), steps=10, hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_image(img):\n",
    "    img_name = img.split(\".\")[-3]\n",
    "    if img_name == \"cat\":\n",
    "        return [1,0]\n",
    "    elif img_name == \"dog\":\n",
    "        return [0,1]\n",
    "\n",
    "#This Function will Return a Vector for the labels of the Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
